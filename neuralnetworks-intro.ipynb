{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><i>Neural Networks</i>:  <u>What, Why, How</u></center>\n",
    "### <center> <u>A Short Introduction</u> </center>\n",
    "#### <center><u> By Aadit Kapoor </u> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning/Deep Learning?\n",
    "<p>\n",
    "    Machine Learning is learning from data. The mathematical intuition behind machine learning is to map a function y = f(x) given y and x. We have to approximate f(x) so that the loss is as low as possible. Loss means the error.\n",
    "   </p>\n",
    "    \n",
    " <img src = \"https://qph.fs.quoracdn.net/main-qimg-6b0eba48ed7424dde2d418349e910c8a-c\"> <br>\n",
    " <img src =\"https://hbr.org/resources/images/article_assets/2016/10/W161026_NG_WHATMACHINEv2.png\">\n",
    " \n",
    " ## Machine Learning problem types\n",
    " - <img src = \"https://i0.wp.com/dataaspirant.com/wp-content/uploads/2014/09/Classification-and-Regression-dataaspirant.png?resize=690%2C518\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Python and Numpy (Quick Primer) </center><br>\n",
    "\n",
    "<p>\n",
    "    Python is a simple and an easy to use programming language.<br>\n",
    "    Numpy is the python library for scientific computing.<br>\n",
    "    You can install numpy using pip.<br>\n",
    "    Pip is similar to Cocoapods and is used to install third party packages in your projects.<br>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <b> Download Python and PIP here: https://www.python.org/ </b>\n",
    "    and then in the terminal you can install the packages:<br>\n",
    "    <b> pip3 install numpy </b><br>\n",
    "    <b> pip3 install keras </b><br>\n",
    "    <b> pip3 install torch </b>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "great!\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "bread\n",
      "100\n",
      "200\n",
      "300\n",
      "500\n",
      "600\n",
      "100\n",
      "500\n",
      "aadit\n"
     ]
    }
   ],
   "source": [
    "# To print\n",
    "print (\"Hello World\")\n",
    "\n",
    "# Define a function\n",
    "def sum(a,b):\n",
    "    return a+b\n",
    "\n",
    "# Conditional statements\n",
    "name = \"aadit\"\n",
    "if name == \"aadit\":\n",
    "    print (\"great!\")\n",
    "else:\n",
    "    print (\"okay!\")\n",
    "    \n",
    "    \n",
    "# Loops\n",
    "for i in range(5): # This will run for five times\n",
    "    print (i)\n",
    "# This is the infinite loop\n",
    "#while True:\n",
    "#   pass\n",
    "\n",
    "# Python also provides dictionaries (key value pair)\n",
    "cost = {\"2000\": \"bread\", \"1000\": \"juice\", \"5000\": \"food\"}\n",
    "print (cost[\"2000\"]) # We can access the elements like this.\n",
    "\n",
    "\n",
    "# Python arrays are called lists\n",
    "price = [100,200,300,500, 600, 100,500, 600,100, 800, 100]\n",
    "\n",
    "print (price[0]) # This will access the first element\n",
    "print (price[1]) # This will access the second element\n",
    "print (price[2]) # and so on...\n",
    "print (price[3])\n",
    "print (price[4])\n",
    "print (price[5])\n",
    "print (price[6])\n",
    "\n",
    "\n",
    "# To define a class\n",
    "class Person:\n",
    "    def __init__(self,name): # This is the constructor (self is the instance of the class.)\n",
    "        self.name = name\n",
    "    def say(self):\n",
    "        print (self.name)\n",
    "\n",
    "        \n",
    "person = Person(\"aadit\") # You create an instance of the class\n",
    "person.say() # calling the function\n",
    "\n",
    "# Inheritance is also supported\n",
    "# class Name(base_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In Python the indentation is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now let us see how to use Numpy\n",
    "\n",
    "<p>\n",
    "    \n",
    "    Numpy is a package for scientifc computing. Using numpy we can perform linear algebra (matrix multiplication etc)<br>\n",
    "    Numpy arrays are faster that regular Python arrays.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[4 5 6 7]\n",
      "[[0 1 2 3]\n",
      " [4 5 6 7]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0.66772435 0.49314048 0.01338067 0.66895993 0.94599639]]\n",
      "Shapes\n",
      "=========\n",
      "(4,)\n",
      "(4,)\n",
      "(2, 4)\n",
      "(5, 4)\n",
      "[0 1 2 3]\n",
      "[4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # import is used to import a package\n",
    "\n",
    "a = np.array([0,1,2,3]) # a vector\n",
    "b = np.array([4,5,6,7]) # another vector\n",
    "c = np.array([[0,1,2,3], # a matrix\n",
    "              [4,5,6,7]])\n",
    "\n",
    "d = np.zeros((5,4)) # (5x4 matrix of zeros)\n",
    "e = np.random.rand(1,5) # random 1x5\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "\n",
    "print (\"Shapes\")\n",
    "print (\"=========\")\n",
    "# (row, column)\n",
    "print(a.shape)\n",
    "print (b.shape)\n",
    "print (c.shape)\n",
    "print (d.shape)\n",
    "\n",
    "print (a.T)\n",
    "print (b.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A shape in a numpy array is nothing but the order of the vector in the form (rows X columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3]\n",
      "[[0.  0.2 0.4 0.6]\n",
      " [0.8 1.  1.2 1.4]]\n",
      "[ 0  5 12 21]\n",
      "[0.  1.  2.4 4.2]\n",
      "[[ 0  1  4  9]\n",
      " [ 0  5 12 21]]\n"
     ]
    }
   ],
   "source": [
    "print(a * 0.1) # multiplies every number in vector \"a\" by 0.1\n",
    "      \n",
    "print(c * 0.2) # multiplies every number in matrix \"c\" by 0.2\n",
    "      \n",
    "print(a * b) # multiplies elementwise between a and b (columns paired up)\n",
    "      \n",
    "print(a * b * 0.2) # elementwise multiplication then multiplied by 0.2\n",
    "      \n",
    "print(a * c) # since c has the same number of columns as a, this performs\n",
    "# elementwise multiplication on every row of the matrix \"c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Index\n",
    "- Neural Networks - What\n",
    "- Neural Networks - Why\n",
    "- Neural Networks - How\n",
    "- Neural Networks - Solving a simple classification problem\n",
    "- Neural Networks - Conversion into Core ML\n",
    "- A short intro to other machine learning frameworks (Scikit Learn, Tensorflow, Keras, Tensorflow For Swift)\n",
    "- Thank You! Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's us talk about what are neural networks?\n",
    "- Machine Learning Algorithm (Like SVM, Decision Tree etc)\n",
    "- Performs well when given a lot of data (Deep Learning)\n",
    "- Loosely based on how the brain works\n",
    "\n",
    "#### History\n",
    "- <p><font size=3>\n",
    "Warren McCulloch and Walter Pitts[2] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.[3]</font></p>\n",
    "<br>\n",
    "Paper: http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- <p><font size=3>\n",
    "    The perceptron algorithm was invented in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt,[3] funded by the United States Office of Naval Research.[4] The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the \"Mark 1 perceptron\". This machine was designed for image recognition: it had an array of 400 photocells, randomly connected to the \"neurons\". Weights were encoded in potentiometers, and weight updates during learning were performed by electric motors.\n",
    "</font></p>\n",
    "<br>\n",
    "Paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf\n",
    "<br>\n",
    "<br>\n",
    "### Neural Networks\n",
    "<p>\n",
    "    <img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png\">\n",
    "- Given above is a neural network with one hidden layer, 3 inputs and 2 outputs.\n",
    "- Each layer has a bias node (circle) except the output layer.\n",
    "- This is a fully connected i.e everything is connected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"http://i.imgur.com/BRA556n.jpg\">\n",
    "- <center><b>Mcculloch and Pitts</b></center>\n",
    "<img width=600 height=600 src = \"https://pbs.twimg.com/media/DIJ13gIXUAAqzAc.jpg\">\n",
    "- <center><b>Geoffrey Hinton</b></center>\n",
    "<img src = \"http://uploads.edubilla.com/inventors/67/9b/frankrosenblatt.png\">\n",
    "- <center><b>Frank Rosenblatt (Perceptron)</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Neural Networks?\n",
    "- Has remarkable ability to derive meaning from complicated  data\n",
    "- Can be used to extract patterns and detect trends that are too complex to be noticed by either humans or computers\n",
    "- A trained neural network can be thought of as an \"expert\" in the category of information it has been given to analyse\n",
    "- This expert can then be used to provide projections given new situations of interest and answer \"what if\" questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of a ANN\n",
    "\n",
    "#### Components of a neural network\n",
    "- Neurons (Nodes) (Circles)\n",
    "- Weights\n",
    "- Activation Functions (ReLU, tanh, sigmoid, leaky ReLU, selu, elu, etc)\n",
    "- Loss Function to minimize (also called cost function) (eg: crossentropy, mse, mae etc)\n",
    "- An optimization technique (Adam (form 1), Gradient Descent, RMSProp etc)\n",
    "- The NN employs an algorithm called <b>backpropagation to calculate the gradient (derivative) of the loss (cost) function with respect to the model parameters (weights and bias)</b> then an optimize algorithm is used to get the direction of the descent and the parameters are updated.\n",
    "- Backprop is basically using the chain rule cleverly. \n",
    "- The output neurons are employed with a activation functions typically a softmax (log softmax) or sigmoid.\n",
    "- Common problems faced by neural nets are vanishing gradient problem and exploding gradient problem.\n",
    "<br>\n",
    "- <b> Backprop paper (1986): https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers in a neural network.\n",
    "<p>\n",
    "   A basic neural network is defined as having an input layer, some hidden layers and an output layer.<br>\n",
    "    The fully connected (the one you saw above) applies a affine transformation (wx+b). The main idea about neural network is that we update the parameters so that the prediction is as close to the desired output. This is done by changing the weights and biases. A neural network is basically a huge composite function (f(g(h(x))). For any machine learning algorithm our job is to map a function y = f(x) (to find the f(x) given x and y) (input and output).<br><br>\n",
    "    A neural network perform a weighted sum (sigma(wx+b) and performs an activation to pass the output onto to the next layer. \n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "- http://www.math.usm.edu/lambers/mat419/lecture10.pdf\n",
    "<img src = \"data:image/png;base64,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\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The main idea about the algorithm is that we need the reach the local minimum (where the loss is as low as possible).\n",
    "- To do this we first randomly intialize the weight and descent in direction negative or positive of the gradient.\n",
    "- new_weight = weight - (alpha * gradient)\n",
    "- Gradient is the derivative of the loss calculated with respect to the parameters of the model. Alpha (hyperparameter) is the learning rate that basically tells how fast we should go (default value is < 1)\n",
    "-  Gradients are calculated using the backprop alogrithm.\n",
    "- There are two passes done in a neural network (forward pass and backward pass), the error is calculated at the output neuron and weights are updated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "<b>The need for activation functions</b><br>\n",
    "- We use an activation for introducing a non linearity in a network.\n",
    "- The output of a neuron i.e activation(wx + b) where activation is the function (eg: sigmoid, ReLU, tanh etc)\n",
    "- When the data is not linearly separable, we have to use a activation function (for example in an XOR problem).\n",
    "- We also use activation functions to limit the output of a neuron (say between 0 and 1 => sigmoid).\n",
    "- When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator.\n",
    "- The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.\n",
    "- If there were no activation functions then the output will always be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some activation functions.\n",
    "<img height = 800 width= 800 src = \"https://cdn-images-1.medium.com/max/1600/1*p_hyqAtyI8pbt2kEl6siOQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some illustrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- <img height=500 width=500 src = \"https://i.stack.imgur.com/zn8uL.png\"><br>\n",
    "- Sample problem neural network ( the first nodes are the inputs)\n",
    "\n",
    "- <img height=600 width=600 src = \"https://i.stack.imgur.com/76Kuo.png\"><br>\n",
    "- Inner working of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning is just a neural network with lots of data and lots of hidden layers.\n",
    "<img height=600 width = 600 src = \"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/08/Why-Deep-Learning-1024x742.png\">\n",
    "- Older algorithms include (svm, decision tree etc)\n",
    "- Why is Deep Learning booming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "<img src = \"http://srdas.github.io/DLBook/DL_images/TNN1.png\"><br>\n",
    "- We have the reach the green point.\n",
    "## Linear Regression using Gradient Descent\n",
    "<img src = \"https://i.stack.imgur.com/GN90y.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- (1) is the weighted sum of our neural network. (We have the minimize W and x is our inputs)\n",
    "- (2) is loss function (mse) (prediciton (f(x) - output))^2)\n",
    "- (3) is the derivative of the loss function wrt to weight (w) and bias (b)\n",
    "- (4) is updating the weights and biases using delta rule.\n",
    "<br>\n",
    "- <b> Our goal is to find those of values of w and b for which the loss (l) is minimum. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backprop is also the same except we have a lot parameters for which we need the find the gradients (dL/dW). We do this using the backpropagation algorithm (chain rule) and then we propagate the error signals backward. (updating the weight layer by layer). The process is iterative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Backprop: A short Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- <b>At its essence backpropagation is just a clever application of the chain rule.</b>\n",
    "<img src = \"https://cdn-images-1.medium.com/max/800/1*liFT6yx2iyAfQ8BXKuHSng.png\">\n",
    "- <b>States that if you have 3 functions f, g and h with f being a function of g and g being a function of h then the derivative of f with respect to h is equal to the product of the derivative of f with respect to g and the derivative of g with respect to h.</b>\n",
    "<br>\n",
    "<p>\n",
    "   <img src=\"https://cdn-images-1.medium.com/max/800/1*RkRtJbQVEDfAMib27Kk2CA.png\"> Let us say we have (x,y) training set (m).\n",
    "    <img src = \"https://cdn-images-1.medium.com/max/800/1*UPCOUu6trB1G7xCbCnIfJg.png\">\n",
    "    <b> We have a 3 dim vector of inputs (x), outputs (y) and outputs (g)</b><br>   \n",
    " </p>\n",
    " \n",
    "  <img src = \"https://cdn-images-1.medium.com/max/800/1*Ofoy4OO_vTS_hjIOOa7-SQ.png\">\n",
    "        This is our loss function where g is the prediction and y is desired output.\n",
    " \n",
    "### Objective \n",
    " <br>\n",
    " - Now the main objective of neural network training is minimize the cost function (loss) by changing the each weight.To do this we use gradient descent, but for applying gradient descent we should have the gradient as the weight updation will happen using (w = w - learning_rate * gradient).\n",
    " - So to calculate gradient we we backprop, our objective is to calculate the derivative of the error function with respect to the model parameters.\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/800/1*Dmvp51wrfov-fFJhqjiwwA.png\"> \n",
    "- Using the chain rule we come to this, the derivative of the error with respect to each weight. (z is a function of the output and the weight).\n",
    "- z = w * d(g) as g is activated using the sigmoid function.\n",
    "- <b>g dash is the derivative of the sigmoid function.</b>\n",
    "\n",
    "### Finally...\n",
    "- We have the gradient of the loss function with respect to the weights and now we can apply gradient descent to reach the space where the error is minimum.\n",
    "\n",
    "### Note: This is very short introduction to backprop but it encapsulates all the important points.\n",
    "\n",
    "<b> Images taken: https://towardsdatascience.com/learning-backpropagation-from-geoffrey-hinton-619027613f0 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## More resources to learn about the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Resources to learn more about the subject.\n",
    "- http://neuralnetworksanddeeplearning.com/\n",
    "- http://www.deeplearningbook.org/\n",
    "- By reading the above papers.\n",
    "- https://dl.acm.org/citation.cfm?id=668382\n",
    "- At last by implementing them.\n",
    "- https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.92157&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now let us look at how to implement neural networks in code.\n",
    "- We will be using a combination of Pytorch and Keras.\n",
    "- Deep Learning frameworks provide us with automatic differentiation.\n",
    "- Framework provide us a way to encapsulate the backend code.\n",
    "\n",
    "<p>\n",
    "    I will not go in detail with Pytorch and Keras but the main difference between pytorch and keras is that pytorch is defined by dynamic run, you create the computation graph and can run it on the go. With Tensorflow (Keras) the graph is static i.e you first define the graph and then in a session you can run the graph.\n",
    "  \n",
    "  \n",
    "  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable # For automatic gradient calculation\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "# Others\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split # For splitting data into train set and test set.\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let us first experiment with some activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch (Dynamic Graph)\n",
      "=======================================\n",
      "Sigmoid:  tensor([ 1.])\n",
      "ReLU:  tensor([ 42.])\n",
      "TanH:  tensor([ 1.])\n",
      "Elu tensor([ 42.])\n",
      "Softmax:  tensor([ 1.])\n",
      "\n",
      "Keras (Static Graph)\n",
      "=======================================\n",
      "Sigmoid:  Tensor(\"Sigmoid_7:0\", shape=(1,), dtype=float64)\n",
      "ReLU:  Tensor(\"Relu_5:0\", shape=(1,), dtype=float64)\n",
      "TanH:  Tensor(\"Tanh_5:0\", shape=(1,), dtype=float64)\n",
      "Elu Tensor(\"Elu_5:0\", shape=(1,), dtype=float64)\n",
      "Softmax:  Tensor(\"Reshape_11:0\", shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch (Dynamic Graph)\")\n",
    "print (\"=======================================\")\n",
    "x = torch.Tensor([42]) # A tensor\n",
    "print (\"Sigmoid: \", F.sigmoid(x)) # between 0 and 1 (don't use this)\n",
    "print (\"ReLU: \", F.relu(x)) # max(0,a) (linear) (use this for hidden layers)\n",
    "print (\"TanH: \", F.tanh(x)) # between -1 and 1 (can use this)\n",
    "print (\"Elu\", F.elu(x)) # Linear (experimentation)\n",
    "print (\"Softmax: \", F.softmax(x, dim=0)) # Genrally used to convert output into probabilities in the output layer.\n",
    "print ()\n",
    "print (\"Keras (Static Graph)\")\n",
    "print (\"=======================================\")\n",
    "x = np.array([1.0])\n",
    "print (\"Sigmoid: \", K.sigmoid(x))\n",
    "print (\"ReLU: \", K.relu(x)) # max(0,a) (linear) (use this for hidden layers)\n",
    "print (\"TanH: \", K.tanh(x)) # between -1 and 1 (can use this)\n",
    "print (\"Elu\", K.elu(x)) # Linear (experimentation)\n",
    "print (\"Softmax: \", K.softmax(x)) # Genrally used to convert output into probabilities in the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- A tensor is basically a vector.\n",
    "- Scalar (1d tensor)\n",
    "- List of List (Vector)\n",
    "- n dimension vector (Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1d:  torch.Size([1])\n",
      "2d:  torch.Size([2, 2])\n",
      "3d:  torch.Size([1, 3, 2])\n",
      "tensor([[ 0.3822,  0.9990,  0.7803,  0.2338],\n",
      "        [ 0.8558,  0.8310,  0.9127,  0.2803],\n",
      "        [ 0.3497,  0.5993,  0.0770,  0.2481],\n",
      "        [ 0.5423,  0.4820,  0.7832,  0.5505]])\n",
      "tensor([[ 0.9939,  0.0113,  0.3742,  0.4728,  0.9508],\n",
      "        [ 0.1396,  0.1917,  0.9351,  0.6882,  0.9521],\n",
      "        [ 0.1258,  0.9595,  0.9697,  0.5089,  0.7573],\n",
      "        [ 0.6951,  0.0931,  0.7793,  0.4472,  0.4248],\n",
      "        [ 0.2510,  0.9690,  0.1668,  0.3804,  0.8359],\n",
      "        [ 0.1346,  0.6283,  0.9535,  0.5691,  0.7349],\n",
      "        [ 0.4912,  0.0603,  0.0962,  0.3202,  0.1519],\n",
      "        [ 0.7144,  0.0457,  0.7851,  0.5493,  0.3653],\n",
      "        [ 0.1516,  0.5434,  0.3423,  0.7927,  0.1125],\n",
      "        [ 0.2801,  0.3594,  0.3292,  0.5315,  0.8926]])\n"
     ]
    }
   ],
   "source": [
    "# We can also do this with numpy.\n",
    "print (\"1d: \", torch.Tensor([1]).shape)\n",
    "print (\"2d: \", torch.Tensor([[1,1],[1,1]]).shape)\n",
    "print (\"3d: \", torch.Tensor([[[1,1],[1,1],[1,1]]]).shape)\n",
    "\n",
    "\n",
    "# A tensor of 4 x 4\n",
    "print (torch.rand(4,4))\n",
    "print (torch.rand(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now let us calculate some gradients (differentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a variable x\n",
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "# Variable is a wrapper around Tensor, requires grad provides that the system has to calculate gradient with respect to this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_x:  tensor([ 3.])\n"
     ]
    }
   ],
   "source": [
    "# Building a function\n",
    "f_x = 3 * x * x # f(x) = 3x^2\n",
    "print (\"f_x: \", f_x) # Putting x = 1 in f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating gradient\n",
    "f_x.backward() # wrt to x\n",
    "# Hence d(f(x)) = 6x\n",
    "# Putting x = 1, we get 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient value:  6.0\n"
     ]
    }
   ],
   "source": [
    "# Checking gradient value\n",
    "print (\"Gradient value: \", x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarly we can calculate derivatives for other functions.<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creation of a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "input = 5\n",
    "hidden = 10\n",
    "output = 2\n",
    "\n",
    "# PyTorch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input, hidden, output):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(input, hidden)\n",
    "        self.l2 = nn.Linear(hidden, output)\n",
    "        # A single hidden layer network\n",
    "    def forward(self, x):\n",
    "        # Getting output from first layer and passing it to the next layer.\n",
    "        out = F.relu(self.l1(x)) # Pre Activation\n",
    "        out = self.l2(out)  # We can also use F.softmax(out, dim=1) depending on the loss function.\n",
    "        return out\n",
    "\n",
    "# Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden, input_dim=input, activation=\"relu\"))\n",
    "model.add(Dense(output, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (l2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 82\n",
      "Trainable params: 82\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Printing out summary\n",
    "net = Net(input, hidden, output)\n",
    "print (net)\n",
    "print()\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l1.weight', Parameter containing:\n",
      "tensor([[ 0.2472, -0.1372, -0.1391,  0.2868,  0.4423],\n",
      "        [ 0.1479,  0.2254,  0.1169,  0.0628,  0.2105],\n",
      "        [ 0.4077,  0.0404, -0.0364,  0.1731, -0.1262],\n",
      "        [ 0.0456,  0.1840,  0.0185,  0.1073,  0.1492],\n",
      "        [ 0.3188,  0.1847, -0.4392,  0.1537,  0.2711],\n",
      "        [-0.2938,  0.0347,  0.3738,  0.2667,  0.0344],\n",
      "        [ 0.1583, -0.3155, -0.4176, -0.2127, -0.1567],\n",
      "        [ 0.4094, -0.2945,  0.1113, -0.4440, -0.0841],\n",
      "        [ 0.4044,  0.2970,  0.0721,  0.1813,  0.4140],\n",
      "        [-0.4127, -0.2336, -0.0218, -0.2896, -0.1163]]))\n",
      "('l1.bias', Parameter containing:\n",
      "tensor([ 0.3519,  0.0010, -0.4226,  0.2627,  0.2823, -0.4133, -0.3418,\n",
      "         0.0641,  0.4234, -0.3400]))\n",
      "('l2.weight', Parameter containing:\n",
      "tensor([[ 0.2574, -0.1282,  0.2090,  0.2112,  0.3162, -0.1087, -0.0155,\n",
      "          0.1115, -0.1288, -0.1724],\n",
      "        [-0.0457, -0.0895, -0.0754, -0.1052,  0.2823,  0.0091, -0.1204,\n",
      "         -0.1686, -0.1477,  0.2817]]))\n",
      "('l2.bias', Parameter containing:\n",
      "tensor([ 0.2481, -0.2912]))\n"
     ]
    }
   ],
   "source": [
    "# Printing weights (randomly assigned)\n",
    "for param in net.named_parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.5276527 , -0.26292667, -0.15418386,  0.39451522, -0.06164157,\n",
       "          0.3849358 , -0.32673505, -0.3841958 , -0.17214736, -0.10469669],\n",
       "        [-0.24589986,  0.08675081, -0.41050506,  0.1896193 ,  0.07890195,\n",
       "         -0.3438062 ,  0.60974914, -0.47975755,  0.29143715, -0.22726247],\n",
       "        [ 0.06937325,  0.56571954,  0.08736163, -0.629089  , -0.06367946,\n",
       "          0.12157995,  0.20947611,  0.2761436 , -0.1319462 , -0.26113635],\n",
       "        [ 0.18716145, -0.61267304,  0.09599721, -0.10066444, -0.28119674,\n",
       "         -0.19047973, -0.2537911 , -0.1822795 ,  0.06281561, -0.52425027],\n",
       "        [ 0.37544662, -0.13515383,  0.08927745,  0.12276739,  0.54750544,\n",
       "          0.29646957,  0.48087114,  0.11858374, -0.14628658, -0.4793756 ]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.4600312 ,  0.6465495 ],\n",
       "        [-0.11415017, -0.57420105],\n",
       "        [-0.42226726, -0.18428642],\n",
       "        [ 0.36069387, -0.58035505],\n",
       "        [ 0.09534305,  0.3909176 ],\n",
       "        [ 0.27903455,  0.13104397],\n",
       "        [-0.5756513 , -0.5331563 ],\n",
       "        [-0.4728182 , -0.48940668],\n",
       "        [ 0.66696566,  0.40292054],\n",
       "        [ 0.05157596,  0.389517  ]], dtype=float32),\n",
       " array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights() # Getting weights (randomly assigned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us solve a simple classification problem with nn's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Creating data\n",
    "data = make_classification(n_samples=50000, n_features=10) # A sample with 50000 records and 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38906299 -0.8470888   1.20317219 ... -2.77786198 -1.06957387\n",
      "  -0.82307484]\n",
      " [-1.13271081 -0.93484106  0.53226557 ...  1.57993383  0.41133052\n",
      "  -0.18182803]\n",
      " [ 0.89384069  1.54612058 -1.70489036 ...  0.96476255  1.36755476\n",
      "  -0.91596387]\n",
      " ...\n",
      " [ 0.35133558  0.3460963  -0.41445323 ...  0.24084999 -0.51595742\n",
      "  -0.5762611 ]\n",
      " [-0.25975114 -1.37364499  1.91941122 ... -0.07888223 -0.62843198\n",
      "   1.85545095]\n",
      " [ 0.57393     0.30496682 -0.0028945  ... -1.52162151  0.48342614\n",
      "   0.30672199]]\n",
      "[0 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# features are the inputs and labels are the outputs (x,y) in a supervised learning experiment.\n",
    "features, labels = data[0],data[1]\n",
    "print (data[0])\n",
    "print (data[1])\n",
    "# Splitting data into training and testing dataset.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking size (row,columns)\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 37500 records in the train set and 12500 records in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "input = features_train.shape[1] # The number of columns (10)\n",
    "hidden = 100\n",
    "output = 2 # 2 (the output)\n",
    "\n",
    "\n",
    "# Let us build the model. (Pytorch)\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    # hidden is a randomly chosen number which specifies the number of hidden nodes in a network. (Depends on the problem)\n",
    "    def __init__(self, input, hidden, output):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(input, hidden)\n",
    "        self.l2 = nn.Linear(hidden, hidden)\n",
    "        self.l3 = nn.Linear(hidden, hidden)\n",
    "        self.l4 = nn.Linear(hidden, output)\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # As a rule of thumb we use relu on hidden layers. (On a more deeper note relu helps us reduce the vanishing gradient problem arised from sigmoid)\n",
    "        out = F.relu(self.l1(x))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = F.relu(self.l3(out))\n",
    "        out = self.l4(out)\n",
    "        return out\n",
    "        \n",
    "model = Model(input, hidden, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model is built, let us start training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "# Stochastic gradient descent\n",
    "# Adam is a variant of gradient descent.\n",
    "# We can change the lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate and model.parameters are the trainable parameters.\n",
    "criterion = nn.CrossEntropyLoss() # For classification\n",
    "epochs = 5 # can change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building dataset\n",
    "# Converting into tensors\n",
    "train_dataset = TensorDataset(torch.from_numpy(features_train), torch.from_numpy(labels_train))\n",
    "test_dataset = TensorDataset(torch.from_numpy(features_test), torch.from_numpy(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are splitting our data into batches so that the weight updation occurs after every training example (Stochastic Gradient Descent <=> Mini Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>This is the recommended way (mini batch gradient descent)</u>\n",
    "# <u>Here we are taking batches of data (64) and then updating our weights</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print (\"epoch #\", epoch)\n",
    "        current_loss = 0\n",
    "        for (feature, label) in train_loader:\n",
    "            feature = Variable(feature).float() # requires_grad is False\n",
    "            label = Variable(label).long() # requires_grad is False\n",
    "            prediction = model(feature) # forward loss\n",
    "            loss = criterion(prediction, label) # calculating loss\n",
    "            current_loss+=loss.item() # Adding loss\n",
    "            optimizer.zero_grad() # Zeroing gradients (manual way in pytorch)\n",
    "            loss.backward() # Calculate the gradient\n",
    "            optimizer.step() # Update the weight\n",
    "        print (\"loss after epoch#:\",epoch, \": \", current_loss)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss after epoch#: 0 :  134.94877634570003\n",
      "epoch # 1\n",
      "loss after epoch#: 1 :  134.27247551083565\n",
      "epoch # 2\n",
      "loss after epoch#: 2 :  133.40212597697973\n",
      "epoch # 3\n",
      "loss after epoch#: 3 :  133.03710904717445\n",
      "epoch # 4\n",
      "loss after epoch#: 4 :  132.65547116845846\n",
      "epoch # 0\n",
      "loss after epoch#: 0:  47.83670485764742\n",
      "epoch # 1\n",
      "loss after epoch#: 1:  47.76737977564335\n",
      "epoch # 2\n",
      "loss after epoch#: 2:  47.869622960686684\n",
      "epoch # 3\n",
      "loss after epoch#: 3:  47.710053242743015\n",
      "epoch # 4\n",
      "loss after epoch#: 4:  47.85326048359275\n"
     ]
    }
   ],
   "source": [
    "def test(epochs):\n",
    "    model.eval()\n",
    "    # No gradients\n",
    "    with torch.no_grad():\n",
    "          for epoch in range(epochs):\n",
    "            print (\"epoch #\", epoch)\n",
    "            current_loss = 0\n",
    "            for (feature, label) in test_loader:\n",
    "                feature = Variable(feature).float() # requires_grad is False\n",
    "                label = Variable(label).long() # requires_grad is False\n",
    "            \n",
    "                prediction = model(feature) # forward pass\n",
    "                loss = criterion(prediction, label) # Calculating loss\n",
    "                current_loss+=loss.item() # Loss after iterative over the whole dataset\n",
    "            print (\"loss after epoch#:\",str(epoch) + \": \", str(current_loss))  \n",
    "            \n",
    "train(epochs)\n",
    "test(epochs)\n",
    "# The prediction portion is same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The loss will decrease on increasing the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For simplicity i will be using the whole data and then updating the weights. (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model structure will be the same. I will only change the optimizer and train, test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numpy arrays into Variable (they remember who created them)\n",
    "\n",
    "# Training dataset\n",
    "x_train = Variable(torch.from_numpy(features_train)).float()\n",
    "y_train = Variable(torch.from_numpy(labels_train)).long()\n",
    "\n",
    "\n",
    "# Testing dataset\n",
    "x_test = Variable(torch.from_numpy(features_test)).float()\n",
    "y_test = Variable(torch.from_numpy(labels_test)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "def train2(epochs):\n",
    "    global losses_train\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_train)\n",
    "        loss = criterion(pred,y_train)\n",
    "        print (\"epoch #\", epoch)\n",
    "        print (\"loss: \", loss.item())\n",
    "        losses_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test = []\n",
    "def test2(epochs):\n",
    "    global losses_test\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(epochs):\n",
    "            pred = model(x_train)\n",
    "            loss = criterion(pred,y_train)\n",
    "            losses_test.append(loss.item())\n",
    "            print (\"epoch #\", epoch)\n",
    "            print (\"loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss:  0.2245241105556488\n",
      "epoch # 1\n",
      "loss:  0.22425973415374756\n",
      "epoch # 2\n",
      "loss:  0.22396187484264374\n",
      "epoch # 3\n",
      "loss:  0.22364023327827454\n",
      "epoch # 4\n",
      "loss:  0.22330573201179504\n"
     ]
    }
   ],
   "source": [
    "train2(epochs) # todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2245241105556488,\n",
       " 0.22425973415374756,\n",
       " 0.22396187484264374,\n",
       " 0.22364023327827454,\n",
       " 0.22330573201179504]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'loss')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the training loss\n",
    "plt.plot(losses_train)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the loss is decreasing over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On unseen data\n",
    "predictions = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the highest probability index (log_softmax)\n",
    "_,predictions = torch.max(predictions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of the model:  0.88552\n"
     ]
    }
   ],
   "source": [
    "print (\"the accuracy of the model: \", accuracy_score(labels_test, predictions.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not bad at all! We can tune the hyperparameters (lr, optimzier etc) to get a better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will build the same model in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Dense(100, input_dim=10, activation=\"relu\"))\n",
    "net.add(Dense(100, activation=\"relu\"))\n",
    "net.add(Dense(100, activation=\"relu\"))\n",
    "net.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# loss is cross entropy\n",
    "net.compile(optimizer=\"adam\", loss=keras.losses.categorical_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing training data into one hot encoding form \n",
    "labels_train_keras = to_categorical(labels_train)\n",
    "labels_test_keras = to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "37500/37500 [==============================] - 3s 74us/step - loss: 0.3364 - acc: 0.8574\n",
      "Epoch 2/5\n",
      "37500/37500 [==============================] - 3s 70us/step - loss: 0.3185 - acc: 0.8666\n",
      "Epoch 3/5\n",
      "37500/37500 [==============================] - 3s 67us/step - loss: 0.3154 - acc: 0.8672\n",
      "Epoch 4/5\n",
      "37500/37500 [==============================] - 2s 67us/step - loss: 0.3130 - acc: 0.8684\n",
      "Epoch 5/5\n",
      "37500/37500 [==============================] - 3s 67us/step - loss: 0.3116 - acc: 0.8688\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model (training)\n",
    "history = net.fit(features_train, labels_train_keras, epochs=5) # todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let us predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:  0.86496\n"
     ]
    }
   ],
   "source": [
    "predictions = net.predict(features_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print (\"accuracy is: \", accuracy_score(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can get a different answer because Keras provides us with everything, it operates like a black box. Everything is configured already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let us convert the model into a Core ML model\n",
    "## To install coremltools use pip. <br>pip install coremltools (pip is a python package manager like cocoapods)\n",
    "<br>\n",
    "## For Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To convert a model we use coremltools provided by Apple. (for keras)\n",
    "- For pytorch Apple has not provided an official tool but we can use onnx built by microsoft and facebook to convert our models into coreml (Pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For keras\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from coremltools.converters.keras import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features1',\n",
       " 'features2',\n",
       " 'features3',\n",
       " 'features4',\n",
       " 'features5',\n",
       " 'features6',\n",
       " 'features7',\n",
       " 'features8',\n",
       " 'features9',\n",
       " 'features10']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_names = []\n",
    "for i in range(1, 11):\n",
    "    input_names.append(\"features\" + str(i))\n",
    "input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****\n",
    "net_saved = convert(net,input_names=input_names, output_names='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_saved.author = \"Aadit Kapoor\"\n",
    "net_saved.short_description = \"demo\"\n",
    "net_saved.license = \"MIT\"\n",
    "net_saved.save(\"demo.mlmodel\")\n",
    "# Model is saved (will take a lot of time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_coreml import convert\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (l2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (l3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (l4): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = Variable(torch.FloatTensor(37500, 10))\n",
    "torch.onnx.export(model, dummy, 'demomodel.proto', verbose=True)\n",
    "model = onnx.load('demomodel.proto')\n",
    "coreml_model = convert(\n",
    "    model,\n",
    "    'classifier',\n",
    "    image_input_names=['features'],\n",
    "    image_output_names=['labels'],\n",
    "    class_labels=[0,1],\n",
    ")\n",
    "coreml_model.save(\"demomodel.mlmodel\")\n",
    "# Model will be saved (*****)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Some real world examples</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nutrition Model \n",
    "- http://localhost:8888/notebooks/Documents/swift-delhi-talk/Calorie%20Predictor/ml-nutrition-database.ipynb\n",
    "### 2. Tic Tac Toe Model\n",
    "- http://localhost:8888/notebooks/Documents/swift-delhi-talk/tic-tac-toe-ml-project/ml-model/tic-tac-toe.ipynb\n",
    "### 3. IPL Match predictor\n",
    "- http://localhost:8888/notebooks/Documents/swift-delhi-talk/ipl-match/winner-predictor.ipynb\n",
    "- http://ipl-predictor.herokuapp.com/home/\n",
    "### 4. Handwriting Model (mnist)\n",
    "- http://localhost:8888/edit/Documents/swift-delhi-talk/mnist-model/mnist-without-cnn.py\n",
    "### 5. Tensorflow for Swift\n",
    "- https://www.tensorflow.org/community/swift\n",
    "### 6. Machine Learning at Apple\n",
    "- http://machinelearning.apple.com/\n",
    "### 7. Turicreate (Turi Create simplifies the development of custom machine learning models\n",
    "- https://github.com/apple/turicreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Thank you! Questions?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i>If you need any help contact me at: aaditkapoor2000@gmail.com.</i>\n",
    "## <i>I am 18 years old  and going to start college in August (Just completed my 12th grade this March, so I am free for collaborations :)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
